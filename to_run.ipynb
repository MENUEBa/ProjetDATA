{"cells":[{"cell_type":"code","source":["pip install PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klgg_M5eQdE-","executionInfo":{"status":"ok","timestamp":1710341549641,"user_tz":-60,"elapsed":12466,"user":{"displayName":"Zoë Butti-Garnier","userId":"07037091739713934829"}},"outputId":"d5210676-ebc1-44c8-e48a-0578ba339f38"},"id":"klgg_M5eQdE-","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m174.1/232.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dshgldJLzgj5","executionInfo":{"status":"ok","timestamp":1710341549641,"user_tz":-60,"elapsed":16,"user":{"displayName":"Zoë Butti-Garnier","userId":"07037091739713934829"}}},"outputs":[],"source":["# Pour récupérer les offres d'emploi qui matchent avec mon CV\n","def data_offres(db):\n","\n","    import json\n","\n","    with open(db, \"r\") as file:\n","      data = json.load(file)\n","\n","    # Liste pour stocker les tuples (ID, description)\n","    Offres = []\n","\n","    # Parcourir chaque emploi dans les résultats\n","    for emploi_resultat in data['resultats']:\n","        emploi_idV = emploi_resultat.get('id', None)\n","        emploi_id = emploi_resultat.get('intitule', None)\n","        description = emploi_resultat.get('description', None)\n","\n","        if emploi_id is not None and description is not None:\n","            Offres.append([emploi_idV, emploi_id, description])\n","\n","    return Offres\n","\n","def extract_text_from_pdf(pdf_file: str) -> [str]:\n","    import PyPDF2\n","    import json\n","    import http.client\n","    with open(pdf_file, 'rb') as pdf:\n","        reader = PyPDF2.PdfReader (pdf, strict=False)\n","        pdf_text = []\n","\n","        for page in reader.pages:\n","            content = page.extract_text()\n","            pdf_text.append (content)\n","        return pdf_text\n","\n","def convert_CV(lien):\n","    import re\n","\n","    extracted_text = extract_text_from_pdf(lien)\n","    CV_List = []\n","\n","    for text in extracted_text:\n","        CV_List.append(text)\n","\n","    CV_List_s = CV_List[0].splitlines()\n","\n","    #Cette partie de l'algorithme est présente pour spliter les informations directement\n","    texte_seul = ' '.join(CV_List_s)  # Convertir la liste en une seule chaîne de texte\n","    texte_seul = re.sub(r'[^a-zA-ZÀ-ÖØ-öø-ÿ\\s]', '', texte_seul)\n","    texte_seul = texte_seul.split()\n","    return texte_seul\n","\n","def biblio_CV(lien):\n","\n","    from gensim import corpora\n","    from gensim.models import LdaModel\n","    from pprint import pprint\n","\n","    # Exemple de corpus de documents\n","    documents = convert_CV(lien)\n","\n","    # Tokenisation des documents en mots\n","    tokenized_documents = [doc.split() for doc in documents]\n","\n","    # Création d'un dictionnaire à partir du corpus\n","    dictionary = corpora.Dictionary(tokenized_documents)\n","\n","    # Création de la représentation du corpus en tant que sac de mots (BoW)\n","    corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n","\n","    # Création d'un modèle LDA\n","    lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary)\n","\n","    return lda_model, dictionary\n","\n","def classifier_offres_lda(lien, top_n, db):\n","\n","    import json\n","\n","    # Liste pour stocker les résultats\n","    Offres=data_offres(db)\n","    resultats_classification = []\n","    resultats_id=[]\n","\n","    lda_model, dictionary = biblio_CV(lien)\n","\n","    # Parcourir chaque offre dans la liste Offres\n","    for offre_idV, offre_id, offre_description in Offres:\n","        # Tokenisation du document\n","        tokenized_doc = offre_description.split()\n","\n","        # Convertir le document en une représentation BoW à l'aide du dictionnaire existant\n","        doc_bow = dictionary.doc2bow(tokenized_doc)\n","\n","        # Obtention de la distribution des topics pour le document\n","        topic_distribution = lda_model.get_document_topics(doc_bow)\n","\n","        # Calcul de la moyenne des scores pour chaque topic\n","        moyenne_scores = sum(score for topic, score in topic_distribution) / len(topic_distribution)\n","\n","        # Ajout du tuple (ID de l'offre, moyenne des scores) à la liste des résultats\n","        resultats_classification.append({\n","            \"Classement\": 0,\n","            \"ID de l'offre\": offre_idV,\n","            \"Intitulé de l'offre\": offre_id,\n","            \"Score\": moyenne_scores\n","        })\n","\n","    # Trier les offres par score de la meilleure à la moins bonne\n","    top_offres = sorted(resultats_classification, key=lambda x: x[\"Score\"], reverse=True)[:top_n]\n","\n","    for i, offre in enumerate(top_offres, start=1):\n","        offre[\"Classement\"] = i\n","        resultats_id.append(offre[\"ID de l'offre\"])\n","\n","    # Enregistrer la liste des offres au format JSON\n","    nom_fichier_sortie_json = \"classement_offres.json\"\n","    with open(nom_fichier_sortie_json, 'w', encoding='utf-8') as fichier_sortie_json:\n","        json.dump(top_offres, fichier_sortie_json, ensure_ascii=False, indent=2)\n","\n","    return resultats_id, top_offres"],"id":"dshgldJLzgj5"},{"cell_type":"code","source":["# Pour identifier les compétences qui me manquent pour ces offres d'emploi\n","\n","# data_competence recupère les compétences associées aux offres d'emploi qui me sont proposées\n","def data_competence(id_emploi, db):\n","    import json\n","\n","    with open(db, \"r\") as file:\n","      data = json.load(file)\n","\n","    # Liste pour stocker les tuples (ID, description)\n","    Competences = []\n","\n","    # Parcourir chaque emploi dans les résultats\n","    for id in id_emploi:\n","        Comp1=[id]\n","        for emploi_resultat in data['resultats']:\n","            emploi_id = emploi_resultat.get('id', None)\n","            if emploi_id == id:\n","                emploi_competences = emploi_resultat.get('competences', None)\n","                if emploi_competences is not None:\n","                    for emploi_libelle in emploi_competences:\n","                        if 'libelle' in emploi_libelle:\n","                            Comp1.append(emploi_libelle['libelle'])\n","        if len(Comp1)>1:\n","            Competences.append(Comp1)\n","    return Competences\n","\n","# Parmi les compétences nécéssaires, identifie celles qui me manquent\n","def classifier_competence_lda(lien, Competences):\n","    # Liste pour stocker les résultats\n","    resultats_classification = []\n","\n","    lda_model, dictionary = biblio_CV(lien)\n","\n","    # Parcourir chaque competence dans la liste Competences\n","    for competence in Competences:\n","        id_offre=competence[0]\n","        liste_par_offre=[]\n","        liste_comp=competence[1:]\n","        for comp in liste_comp:\n","            # Tokenisation de la chaîne de texte\n","            tokenized_doc = comp.split()\n","\n","            # Convertir le document en une représentation BoW à l'aide du dictionnaire existant\n","            doc_bow = dictionary.doc2bow(tokenized_doc)\n","\n","            # Obtention de la distribution des topics pour le document\n","            topic_distribution = lda_model.get_document_topics(doc_bow)\n","\n","            # Calcul de la moyenne des scores pour chaque topic\n","            moyenne_scores = sum(score for topic, score in topic_distribution) / len(topic_distribution)\n","\n","            # Ajout du tuple (nom de la compétence, moyenne des scores) à la liste des résultats pour cette offre\n","            liste_par_offre.append((comp, moyenne_scores))\n","\n","        # Tri des tuples selon le score croissant\n","        sorted_tuples = sorted(liste_par_offre, key=lambda x: x[1])\n","\n","        # Select the first two tuples and remove the score\n","        top_two_tuples = [t[0] for t in sorted_tuples[:2]]\n","\n","        # Ajout des résultats obtenus pour cette offre à la liste de tous les résultats\n","        resultats_classification.append([[id_offre] + top_two_tuples])\n","\n","    return(resultats_classification)"],"metadata":{"id":"hbt_bqXaS9l6","executionInfo":{"status":"ok","timestamp":1710341549642,"user_tz":-60,"elapsed":12,"user":{"displayName":"Zoë Butti-Garnier","userId":"07037091739713934829"}}},"id":"hbt_bqXaS9l6","execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Identifier les formations qui permettent d'acquérir ces compétences\n","\n","def formation(liste_comp, catalogue_formations):\n","    import pandas as pd\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.metrics.pairwise import cosine_similarity\n","    import json\n","\n","    df = pd.read_csv(catalogue_formations, on_bad_lines='skip', sep=';', low_memory=False)\n","\n","    desired_columns = ['numero_formation', 'intitule_formation', 'points_forts']\n","    df_selected = df[desired_columns]\n","\n","    resultat_du_dictionnaire = df_selected.to_dict(orient='records')\n","\n","    nom_fichier_sortie = \"resultats_formations.json\"\n","\n","    # Liste pour stocker les résultats au format JSON\n","    resultats_json = []\n","\n","    for comp_par_emploi1 in liste_comp:\n","        comp_par_emploi2 = comp_par_emploi1[0]\n","        id_offre, competence_cherche_list = comp_par_emploi2[0], comp_par_emploi2[1:]\n","\n","        # Dictionnaire pour stocker les résultats\n","        resultat_par_competence = {\"Id de l'offre d'emploi\": id_offre, \"Competences\": []}\n","\n","        for competence_cherche in competence_cherche_list:\n","            # Liste des phrases dans resultat_du_dictionnaire\n","            phrases = [str(item.get('points_forts', [])) for item in resultat_du_dictionnaire]\n","\n","            # Ajout de la phrase d'intérêt à la liste\n","            phrases.append(competence_cherche)\n","\n","            # Création de la matrice TF-IDF\n","            vectorizer = TfidfVectorizer()\n","            tfidf_matrix = vectorizer.fit_transform(phrases)\n","\n","            # Calcul de la similarité du cosinus entre les phrases\n","            similarites = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n","\n","            # Obtenir les indices triés des phrases les plus similaires\n","            indices_similaires = similarites.argsort()[0][::-1]\n","\n","            # Limiter le nombre de correspondances à ajouter à la liste de formations\n","            limite_correspondances = 3\n","            formations = [resultat_du_dictionnaire[i] for i in indices_similaires[:limite_correspondances]]\n","\n","            # Stocker les résultats dans le dictionnaire\n","            resultat_competence = {\"Competence\": competence_cherche, \"Formations\": []}\n","\n","            for formacao in formations:\n","                formation_dict = {\n","                    \"Numéro formation\": formacao['numero_formation'],\n","                    \"Intitulé\": formacao['intitule_formation'],\n","                    \"Points forts\": formacao['points_forts']\n","                }\n","                resultat_competence[\"Formations\"].append(formation_dict)\n","\n","            resultat_par_competence[\"Competences\"].append(resultat_competence)\n","\n","        resultats_json.append(resultat_par_competence)\n","\n","    # Enregistrer la liste des résultats au format JSON\n","    with open(nom_fichier_sortie, 'w', encoding='utf-8') as fichier_sortie_json:\n","        json.dump(resultats_json, fichier_sortie_json, ensure_ascii=False, indent=2)\n","\n","    return resultats_json"],"metadata":{"id":"8q4rlCL-WQLB","executionInfo":{"status":"ok","timestamp":1710341549981,"user_tz":-60,"elapsed":8,"user":{"displayName":"Zoë Butti-Garnier","userId":"07037091739713934829"}}},"id":"8q4rlCL-WQLB","execution_count":4,"outputs":[]},{"cell_type":"code","source":["def final(CV, database, catalogue_formations):\n","  id_emploi_resultat, classement_offres = classifier_offres_lda(CV, 10, database)\n","  liste_comp=data_competence(id_emploi_resultat, database)\n","  liste_comp_manquantes=classifier_competence_lda(CV, liste_comp)\n","  resultats_formations=formation(liste_comp_manquantes, catalogue_formations)\n","  return classement_offres, resultats_formations"],"metadata":{"id":"qh6AHl_qW0y5","executionInfo":{"status":"ok","timestamp":1710341549982,"user_tz":-60,"elapsed":7,"user":{"displayName":"Zoë Butti-Garnier","userId":"07037091739713934829"}}},"id":"qh6AHl_qW0y5","execution_count":5,"outputs":[]},{"cell_type":"code","source":["pip install gradio"],"metadata":{"id":"pvy12jWHbKvK","executionInfo":{"status":"ok","timestamp":1710341581317,"user_tz":-60,"elapsed":31341,"user":{"displayName":"Zoë Butti-Garnier","userId":"07037091739713934829"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e667f2a-6ef5-4b5e-99df-65024050a5e5"},"id":"pvy12jWHbKvK","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-4.21.0-py3-none-any.whl (17.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.12.0 (from gradio)\n","  Downloading gradio_client-0.12.0-py3-none-any.whl (310 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.3)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.3)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.3.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.28.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.12.0->gradio) (2023.6.0)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.12.0->gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n","Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n","Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n","  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=e71d75e465236b54e7278b50c235c7800b6cb9f453f0e66b5f1188c98ec15cc6\n","  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n","Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.0 ffmpy-0.3.2 gradio-4.21.0 gradio-client-0.12.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 orjson-3.9.15 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tomlkit-0.12.0 uvicorn-0.28.0 websockets-11.0.3\n"]}]},{"cell_type":"code","source":["import gradio as gr\n","import json\n","import http.client\n","from io import BytesIO\n","import tempfile\n","\n","iface = gr.Interface(fn=final, inputs=[\"file\", \"file\", \"file\"], outputs=[\"json\", \"json\"])\n","iface.launch()"],"metadata":{"id":"L6sg52VCbMsa"},"id":"L6sg52VCbMsa","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}